{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LCPs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn   \n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim import Adam\n",
    "from torch.autograd import Variable\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from math import pi, cos\n",
    "from scipy.special import factorial\n",
    "from sympy import symbols, lambdify\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import numpy as np\n",
    "from sympy import simplify\n",
    "from scipy.sparse import diags\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.optim import LBFGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:5\" if torch.cuda.is_available() else \"cpu\")  # Check for CUDA availability\n",
    "\n",
    "m = 2  # degree of polynomial Q(x)\n",
    "n = 2  # degree of polynomial P(x)\n",
    "q = 1  # degree of truncated Taylor series,q<=n\n",
    "J = 16  # for loss_1\n",
    "N = 10000  # for lambda values in loss_1\n",
    "h = 1/N  # for lambda values in loss_1\n",
    "p2 = 1  # penalty term for loss_2\n",
    "epochs = 500\n",
    "K = 500 # loops in one epoch\n",
    "\n",
    "# coefficients of exp(-x) Taylor expansion\n",
    "c = torch.tensor([(-1)**i / factorial(i) for i in range(q+1)], dtype=torch.float32, device=device)\n",
    "\n",
    "# Padé approximation parameters to be learned\n",
    "a2 = nn.Parameter(torch.zeros(n-q, device=device), requires_grad=True)\n",
    "b1_log = nn.Parameter(torch.zeros(q, device=device), requires_grad=True)\n",
    "b2_log = nn.Parameter(torch.ones(m-q, device=device), requires_grad=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(x):\n",
    "    \"\"\"\n",
    "    Polynomial function with coefficients a = [c*B, a2]\n",
    "    \"\"\"\n",
    "    B = torch.cat((torch.ones(1, device=device), torch.exp(b1_log)))\n",
    "    c_B = torch.zeros_like(B, device=device)\n",
    "    for k in range(len(B)):\n",
    "        for j in range(k+1):\n",
    "            c_B[k] += c[j] * B[k-j]\n",
    "    a = torch.cat((c_B, a2))\n",
    "    p = torch.zeros_like(x, device=device)\n",
    "    for coeff in a.flip(0):  # Reverse the tensor for Horner's method\n",
    "        p = p * x + coeff\n",
    "    return p\n",
    "\n",
    "def Q(x):\n",
    "    \"\"\"\n",
    "    Polynomial function with coefficients b = [B, b2]\n",
    "    \"\"\"\n",
    "    B = torch.cat((torch.ones(1, device=device), torch.exp(b1_log)))\n",
    "    b = torch.cat((B, torch.exp(b2_log)))\n",
    "    q = torch.zeros_like(x, device=device)\n",
    "    for coeff in b.flip(0):  # Reverse the tensor for Horner's method\n",
    "        q = q * x + coeff\n",
    "    return q\n",
    "\n",
    "# LCPs\n",
    "def R(x):\n",
    "    return P(x) / Q(x)\n",
    "\n",
    "# two-stage Lobatto IIIC \n",
    "def r2(x):\n",
    "    return (2) / (x ** 2 + 2 * x + 2) \n",
    "\n",
    "# three-stage Lobatto IIIC\n",
    "def r3(x):\n",
    "    return (-6 * x + 24) / (x ** 3 + 6 * x ** 2 + 18 * x + 24)\n",
    "\n",
    "# four-stage Lobatto IIIC\n",
    "def r4(x):\n",
    "    return (12*x**2 - 120*x + 360) / (x**4 + 12*x**3 + 72*x**2 + 240*x + 360)\n",
    "\n",
    "# three-stage Radau IIA\n",
    "def rc(s):\n",
    "    b = 0.5 * (1 + np.sqrt(3) / 3)\n",
    "    return 1 - s / (1 + b * s) - np.sqrt(3) / 6 * (s / (1 + b * s))**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_lambda_zeros(b1, b2, a2):\n",
    "    A = a2 * torch.exp(b1) + torch.exp(b2) - torch.exp(b1 + b2)\n",
    "    B = -2 * (torch.exp(b2) - a2)\n",
    "    C = -1\n",
    "    \n",
    "    discriminant = B**2 - 4 * A * C\n",
    "    \n",
    "    if torch.any(discriminant < 0):\n",
    "        raise ValueError(\"The equation has no real roots for some inputs.\")\n",
    "    \n",
    "    sqrt_discriminant = torch.sqrt(discriminant)\n",
    "    \n",
    "    root1 = (-B + sqrt_discriminant) / (2 * A)\n",
    "    root2 = (-B - sqrt_discriminant) / (2 * A)\n",
    "    \n",
    "    return root1, root2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "lambdas = torch.tensor([-(2*cos(j*pi*h) - 2)/(h**2) for j in range(1, N)], dtype=torch.float32).to(device)\n",
    "lambdas2 = torch.linspace(0.01, 100, 5000, device=device)\n",
    "\n",
    "lambdas = torch.cat((lambdas, lambdas2))\n",
    "\n",
    "# Define the dataset\n",
    "dataset = torch.utils.data.TensorDataset(lambdas)\n",
    "\n",
    "# Define the data loader with batch size equal to the total number of samples\n",
    "data_loader = torch.utils.data.DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "# Define an optimizer with Adam\n",
    "optimizer = torch.optim.SGD([a2, b1_log, b2_log], lr=1e-4)\n",
    "# optimizer = torch.optim.Adam([a2, b1_log, b2_log], lr=0.01)\n",
    "\n",
    "# Define a learning rate scheduler\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=K, eta_min=0)\n",
    "\n",
    "best_params = None\n",
    "best_loss1 = float('inf')\n",
    "\n",
    "# 定义收敛阈值\n",
    "convergence_threshold = 4e-1\n",
    "\n",
    "recent_losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    p2 = p2 * 0.95\n",
    "    converged = False\n",
    "    \n",
    "    for i in range(K):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 计算 loss1\n",
    "        loss1 = torch.max(torch.abs((r4(lambdas/J))**J - R(lambdas))/(1 - torch.abs(R(lambdas))))\n",
    "        # loss1 = torch.max(torch.abs((rc(lambdas/J))**J - R(lambdas))/(1 - torch.abs(R(lambdas))))\n",
    "        # loss1 = torch.max(torch.abs((r3(lambdas/J))**J - R(lambdas))/(1 - torch.abs(R(lambdas))))\n",
    "        # loss1 = torch.max(torch.abs((r2(lambdas/J))**J - R(lambdas))/(1 - torch.abs(R(lambdas))))\n",
    "        # loss1 = torch.max(a2 + b1_log + b2_log)\n",
    "        \n",
    "        # 计算 loss2\n",
    "        root1, root2 = solve_lambda_zeros(b1_log, b2_log, a2)\n",
    "        pos_lambdas = torch.max(torch.max(root1, torch.tensor(0.001, device=device)), torch.max(root2, torch.tensor(0.001, device=device)))\n",
    "        pos_lambdas = torch.cat((pos_lambdas, lambdas))\n",
    "        loss2 = torch.mean(torch.log(1 - R(lambdas)**2) + torch.log(1 - (a2 / torch.exp(b2_log))**2))\n",
    "\n",
    "        # 组合损失\n",
    "        loss = loss1 - p2 * loss2\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # 如果 loss1 小于当前最小值，则更新 best_params\n",
    "        if loss1 < best_loss1:\n",
    "            best_params = (a2.clone().detach(), b1_log.clone().detach(), b2_log.clone().detach())\n",
    "            best_loss1 = loss1.item()\n",
    "\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            # 重新计算 loss1 的梯度\n",
    "            optimizer.zero_grad()\n",
    "            loss1.backward()\n",
    "            \n",
    "            # 计算梯度范数\n",
    "            total_norm = 0\n",
    "            for p in [a2, b1_log, b2_log]:\n",
    "                if p.grad is not None:\n",
    "                    param_norm = p.grad.data.norm(2)\n",
    "                    total_norm += param_norm.item() ** 2\n",
    "            total_norm = total_norm ** 0.5\n",
    "\n",
    "            # 仅在 i=0 时跟踪 recent_losses\n",
    "            if i == 0:\n",
    "                recent_losses.append(loss1.item())\n",
    "                if len(recent_losses) > 3:\n",
    "                    recent_losses.pop(0)\n",
    "\n",
    "            print(f\"Iteration {epoch}, Iteration: {i}, Loss1: {loss1:.6f}, Loss2: {loss2:.6f},gradient norm {total_norm:.6f}, Total Loss: {loss.item():.6f}\")\n",
    "            \n",
    "            # 检查梯度范数以确定收敛性\n",
    "            if total_norm < convergence_threshold:\n",
    "                print(f\"Converged at epoch {epoch}, iteration {i}, gradient norm {total_norm:.6f}\")\n",
    "                converged = True\n",
    "                break\n",
    "            \n",
    "        \n",
    "    \n",
    "    # 检查整个 epoch 是否收敛\n",
    "    if converged:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2.data = best_params[0]\n",
    "b1_log.data = best_params[1]\n",
    "b2_log.data = best_params[2]\n",
    "\n",
    "B = torch.cat((torch.ones(1, device=device), torch.exp(b1_log)))\n",
    "c_B = torch.zeros_like(B, device=device)\n",
    "for k in range(len(B)):\n",
    "    for j in range(k + 1):\n",
    "        c_B[k] += c[j] * B[k - j]\n",
    "\n",
    "a = torch.cat((c_B, a2))\n",
    "b = torch.cat((torch.ones(1, device=device), torch.exp(b1_log), torch.exp(b2_log)))\n",
    "\n",
    "Px = \" + \".join([f\"{a[i].item():.5f}*x**{i}\" for i in range(len(a))])\n",
    "Qx = \" + \".join([f\"{b[i].item():.5f}*x**{i}\" for i in range(len(b))])\n",
    "\n",
    "print(f\"R_x = ({Px}) / ({Qx})\\nThe best convergence rate is {best_loss1:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define symbolic variable\n",
    "x = symbols('x')\n",
    "\n",
    "# Define the function r(x), keeping 4 decimal places\n",
    "R_x = (1.00000*x**0 + -0.21151*x**1 + 0.00500*x**2) / (1.00000*x**0 + 0.78849*x**1 + 0.37864*x**2)\n",
    "\n",
    "# Define the p_i function, replacing r with r(x)\n",
    "p1_x = -(R_x - 1)/x\n",
    "\n",
    "# Simplify p_i\n",
    "p1_simplified = simplify(p1_x)\n",
    "\n",
    "# Output the simplified p_i\n",
    "print(\"p_1(x) =\", p1_simplified)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
